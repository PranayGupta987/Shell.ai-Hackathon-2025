{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de25f23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import FTTransformerConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define columns\n",
    "blend_cols = ['Component1_fraction', 'Component2_fraction', 'Component3_fraction', \n",
    "              'Component4_fraction', 'Component5_fraction']\n",
    "prop_cols = [f'Component{i+1}_Property{j}' for i in range(5) for j in range(1, 11)]\n",
    "target_cols = [f'BlendProperty{i}' for i in range(1, 11)]\n",
    "feature_cols = [col for col in train_df.columns if col not in target_cols]\n",
    "\n",
    "# Verify blend compositions\n",
    "assert np.allclose(train_df[blend_cols].sum(axis=1), 1.0, atol=1e-5), \"Train blend compositions must sum to 1\"\n",
    "assert np.allclose(test_df[blend_cols].sum(axis=1), 1.0, atol=1e-5), \"Test blend compositions must sum to 1\"\n",
    "train_df[blend_cols] = np.clip(train_df[blend_cols], 0, 1)\n",
    "test_df[blend_cols] = np.clip(test_df[blend_cols], 0, 1)\n",
    "\n",
    "# Feature Engineering Functions\n",
    "def compute_weighted_properties(df, blend_cols, prop_cols):\n",
    "    weighted_props = {}\n",
    "    for prop in range(1, 11):\n",
    "        prop_col = [f'Component{i+1}_Property{prop}' for i in range(5)]\n",
    "        weighted_props[f'Weighted_Property{prop}'] = sum(df[col] * df[blend_cols[i]] for i, col in enumerate(prop_col, 0))\n",
    "    return pd.DataFrame(weighted_props)\n",
    "\n",
    "def compute_statistical_aggregates(df, prop_cols):\n",
    "    aggregates = {}\n",
    "    for name, func in zip(['mean', 'std'], [np.mean, np.std]):\n",
    "        aggregates[f'Prop_{name}'] = df[prop_cols].apply(func, axis=1)\n",
    "    return pd.DataFrame(aggregates)\n",
    "\n",
    "def compute_property_min_max(df, prop_cols):\n",
    "    min_max = {}\n",
    "    for prop in range(1, 11):\n",
    "        prop_col = [f'Component{i}_Property{prop}' for i in range(1, 6)]\n",
    "        min_max[f'Min_Property{prop}'] = df[prop_col].min(axis=1)\n",
    "        min_max[f'Max_Property{prop}'] = df[prop_col].max(axis=1)\n",
    "    return pd.DataFrame(min_max)\n",
    "\n",
    "def compute_property_blend_interactions(df, blend_cols, prop_cols):\n",
    "    interactions = {}\n",
    "    for i in range(5):\n",
    "        for j in range(1, 11):\n",
    "            prop_col = f'Component{i+1}_Property{j}'\n",
    "            interactions[f'Interaction_{blend_cols[i]}_{prop_col}'] = df[blend_cols[i]] * df[prop_col]\n",
    "    return pd.DataFrame(interactions)\n",
    "\n",
    "def compute_weighted_squared_properties(df, blend_cols, prop_cols):\n",
    "    weighted_squared = {}\n",
    "    for prop in range(1, 11):\n",
    "        prop_col = [f'Component{i+1}_Property{prop}' for i in range(5)]\n",
    "        weighted_squared[f'Weighted_Squared_Property{prop}'] = sum((df[col] ** 2) * df[blend_cols[i]] for i, col in enumerate(prop_col))\n",
    "    return pd.DataFrame(weighted_squared)\n",
    "\n",
    "def compute_property_deviations(df, blend_cols, prop_cols, weighted_props):\n",
    "    deviations = {}\n",
    "    for i in range(5):\n",
    "        for j in range(1, 11):\n",
    "            prop_col = f'Component{i+1}_Property{j}'\n",
    "            weighted_col = f'Weighted_Property{j}'\n",
    "            deviations[f'Deviation_{blend_cols[i]}_{prop_col}'] = df[blend_cols[i]] * (df[prop_col] - weighted_props[weighted_col])\n",
    "    return pd.DataFrame(deviations)\n",
    "\n",
    "def compute_property_variance(df, blend_cols, prop_cols, weighted_props):\n",
    "    variance = {}\n",
    "    for prop in range(1, 11):\n",
    "        prop_col = [f'Component{i+1}_Property{prop}' for i in range(5)]\n",
    "        weighted_col = f'Weighted_Property{prop}'\n",
    "        variance[f'Variance_Property{prop}'] = sum(\n",
    "            df[blend_cols[i]] * (df[prop_col[i]] - weighted_props[weighted_col]) ** 2 for i in range(5)\n",
    "        )\n",
    "    return pd.DataFrame(variance)\n",
    "\n",
    "train_df = pd.concat([train_df, compute_weighted_properties(train_df, blend_cols, prop_cols),\n",
    "                      compute_statistical_aggregates(train_df, prop_cols),\n",
    "                      compute_property_min_max(train_df, prop_cols),\n",
    "                      compute_property_blend_interactions(train_df, blend_cols, prop_cols),\n",
    "                      compute_weighted_squared_properties(train_df, blend_cols, prop_cols),\n",
    "                      compute_property_deviations(train_df, blend_cols, prop_cols, compute_weighted_properties(train_df, blend_cols, prop_cols)),\n",
    "                      compute_property_variance(train_df, blend_cols, prop_cols, compute_weighted_properties(train_df, blend_cols, prop_cols))], axis=1)\n",
    "test_df = pd.concat([test_df, compute_weighted_properties(test_df, blend_cols, prop_cols),\n",
    "                     compute_statistical_aggregates(test_df, prop_cols),\n",
    "                     compute_property_min_max(test_df, prop_cols),\n",
    "                     compute_property_blend_interactions(test_df, blend_cols, prop_cols),\n",
    "                     compute_weighted_squared_properties(test_df, blend_cols, prop_cols),\n",
    "                     compute_property_deviations(test_df, blend_cols, prop_cols, compute_weighted_properties(test_df, blend_cols, prop_cols)),\n",
    "                     compute_property_variance(test_df, blend_cols, prop_cols, compute_weighted_properties(test_df, blend_cols, prop_cols))], axis=1)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "blend_poly_train = poly.fit_transform(train_df[blend_cols])\n",
    "blend_poly_test = poly.transform(test_df[blend_cols])\n",
    "for i, col in enumerate(poly.get_feature_names_out(blend_cols)):\n",
    "    train_df[f'Poly_{col}'] = blend_poly_train[:, i]\n",
    "    test_df[f'Poly_{col}'] = blend_poly_test[:, i]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(i + 1, 5):\n",
    "        train_df[f'Interaction_{blend_cols[i]}_{blend_cols[j]}'] = train_df[blend_cols[i]] * train_df[blend_cols[j]]\n",
    "        test_df[f'Interaction_{blend_cols[i]}_{blend_cols[j]}'] = test_df[blend_cols[i]] * test_df[blend_cols[j]]\n",
    "        train_df[f'Ratio_{blend_cols[i]}/{blend_cols[j]}'] = train_df[blend_cols[i]] / (train_df[blend_cols[j]] + 1e-6)\n",
    "        test_df[f'Ratio_{blend_cols[i]}/{blend_cols[j]}'] = test_df[blend_cols[i]] / (test_df[blend_cols[j]] + 1e-6)\n",
    "\n",
    "\n",
    "feature_cols = [col for col in train_df.columns if col not in target_cols]\n",
    "\n",
    "def safe_mape(y_true, y_pred, epsilon=1e-6):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + epsilon)))\n",
    "\n",
    "def select_features_per_target(df, target, feature_cols, threshold=0.1):\n",
    "    correlations = df[feature_cols].corrwith(df[target]).abs()\n",
    "    selected_features = correlations[correlations > threshold].index.tolist()\n",
    "    if len(selected_features) < 10:\n",
    "        selected_features = correlations.nlargest(10).index.tolist()\n",
    "    return selected_features\n",
    "\n",
    "class ANNModel(nn.Module):\n",
    "    def _init_(self, input_dim):\n",
    "        super(ANNModel, self)._init_()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_ann_model(X_train, y_train, X_val, y_val, input_dim):\n",
    "    model = ANNModel(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), \n",
    "                                  torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), \n",
    "                                torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_fttransformer_regressor(train_data, val_data, test_df, selected_cols, target, n_folds=5):\n",
    "    \"\"\"\n",
    "    Trains an FTTransformerRegressor for a specific target property using pytorch_tabular.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_data: DataFrame with training data\n",
    "    - val_data: DataFrame with validation data\n",
    "    - test_df: DataFrame with test data\n",
    "    - selected_cols: List of feature columns\n",
    "    - target: Target column name\n",
    "    - n_folds: Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "    - oof_pred: Out-of-fold predictions for validation data\n",
    "    - test_pred: Test predictions averaged over folds\n",
    "    \"\"\"\n",
    "    data_config = DataConfig(\n",
    "        target=[target], continuous_cols=selected_cols, categorical_cols=[],\n",
    "        continuous_feature_transform=\"quantile_normal\", normalize_continuous_features=True\n",
    "    )\n",
    "    trainer_config = TrainerConfig(\n",
    "        auto_lr_find=True, batch_size=32, max_epochs=50, early_stopping_patience=10,\n",
    "        checkpoints=None, load_best=True\n",
    "    )\n",
    "    optimizer_config = OptimizerConfig()\n",
    "    model_config = FTTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        metrics=[\"mean_squared_error\"],\n",
    "        metrics_params=[{}],\n",
    "        learning_rate=0.001,\n",
    "        num_heads=8\n",
    "    )\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config\n",
    "    )\n",
    "    tabular_model.fit(train=train_data, validation=val_data)\n",
    "    oof_pred = tabular_model.predict(val_data).iloc[:, 0].values\n",
    "    test_pred = tabular_model.predict(test_df).iloc[:, 0].values / n_folds\n",
    "    return oof_pred, test_pred\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_mapes = []\n",
    "transformer_oof = np.zeros((len(train_df), len(target_cols)))\n",
    "lgb_oof = np.zeros((len(train_df), len(target_cols)))\n",
    "xgb_oof = np.zeros((len(train_df), len(target_cols)))\n",
    "cat_oof = np.zeros((len(train_df), len(target_cols)))\n",
    "ann_oof = np.zeros((len(train_df), len(target_cols)))\n",
    "transformer_test = np.zeros((len(test_df), len(target_cols)))\n",
    "lgb_test = np.zeros((len(test_df), len(target_cols)))\n",
    "xgb_test = np.zeros((len(test_df), len(target_cols)))\n",
    "cat_test = np.zeros((len(test_df), len(target_cols)))\n",
    "ann_test = np.zeros((len(test_df), len(target_cols)))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_data = train_df.iloc[train_idx].copy()\n",
    "    val_data = train_df.iloc[val_idx].copy()\n",
    "\n",
    "    \n",
    "    train_data[feature_cols] = train_data[feature_cols].fillna(train_data[feature_cols].median())\n",
    "    val_data[feature_cols] = val_data[feature_cols].fillna(train_data[feature_cols].median())\n",
    "    test_df[feature_cols] = test_df[feature_cols].fillna(train_data[feature_cols].median())\n",
    "\n",
    "    feature_sets = {target: select_features_per_target(train_data, target, feature_cols) for target in target_cols}\n",
    "\n",
    "    \n",
    "    for i, target in enumerate(target_cols):\n",
    "        selected_cols = feature_sets[target]\n",
    "        X_train = train_data[selected_cols].values\n",
    "        X_val = val_data[selected_cols].values\n",
    "        X_test = test_df[selected_cols].values\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Train FTTransformerRegressor for this target\n",
    "        oof_pred, test_pred = train_fttransformer_regressor(train_data, val_data, test_df, selected_cols, target)\n",
    "        transformer_oof[val_idx, i] = oof_pred\n",
    "        transformer_test[:, i] += test_pred\n",
    "\n",
    "        # Train and predict with LightGBM\n",
    "        lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.03)\n",
    "        lgb_model.fit(X_train_scaled, train_data[target_cols].values[:, i])\n",
    "        lgb_oof[val_idx, i] = lgb_model.predict(X_val_scaled)\n",
    "        lgb_test[:, i] += lgb_model.predict(X_test_scaled) / kf.n_splits\n",
    "\n",
    "        # Train and predict with XGBoost\n",
    "        xgb_model = XGBRegressor(n_estimators=700, learning_rate=0.03)\n",
    "        xgb_model.fit(X_train_scaled, train_data[target_cols].values[:, i])\n",
    "        xgb_oof[val_idx, i] = xgb_model.predict(X_val_scaled)\n",
    "        xgb_test[:, i] += xgb_model.predict(X_test_scaled) / kf.n_splits\n",
    "\n",
    "        # Train and predict with CatBoost\n",
    "        cat_model = CatBoostRegressor(verbose=0, iterations=700, learning_rate=0.04)\n",
    "        cat_model.fit(X_train_scaled, train_data[target_cols].values[:, i])\n",
    "        cat_oof[val_idx, i] = cat_model.predict(X_val_scaled)\n",
    "        cat_test[:, i] += cat_model.predict(X_test_scaled) / kf.n_splits\n",
    "\n",
    "        # Train and predict with ANN\n",
    "        ann_model = train_ann_model(X_train_scaled, train_data[target_cols].values[:, i].reshape(-1, 1),\n",
    "                                    X_val_scaled, val_data[target_cols].values[:, i].reshape(-1, 1),\n",
    "                                    input_dim=X_train_scaled.shape[1])\n",
    "        ann_model.eval()\n",
    "        with torch.no_grad():\n",
    "            ann_oof[val_idx, i] = ann_model(torch.tensor(X_val_scaled, dtype=torch.float32).to(device)).cpu().numpy().flatten()\n",
    "            ann_test[:, i] += ann_model(torch.tensor(X_test_scaled, dtype=torch.float32).to(device)).cpu().numpy().flatten() / kf.n_splits\n",
    "\n",
    "    # Fold MAPE\n",
    "    fold_mape = np.mean([safe_mape(val_data[target_cols].values[:, i], transformer_oof[val_idx, i]) for i in range(len(target_cols))])\n",
    "    val_mapes.append(fold_mape)\n",
    "    print(f\"  Fold {fold + 1} MAPE: {fold_mape:.5f}\")\n",
    "\n",
    "\n",
    "# Meta-Ensemble using Bayesian Ridge\n",
    "meta_X = np.concatenate([transformer_oof, lgb_oof, xgb_oof, cat_oof, ann_oof], axis=1)\n",
    "meta_test = np.concatenate([transformer_test, lgb_test, xgb_test, cat_test, ann_test], axis=1)\n",
    "meta_model = MultiOutputRegressor(BayesianRidge())\n",
    "meta_model.fit(meta_X, train_df[target_cols].values)\n",
    "final_preds = (  ann_test)\n",
    "# Save submission\n",
    "submission = pd.DataFrame(final_preds, columns=target_cols)\n",
    "submission.insert(0, 'ID', test_df['ID'])\n",
    "submission.to_csv('submission_hybrid.csv', index=False)\n",
    "print(\"Saved submission_hybrid.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
